{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LibiumNetP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NKJhfg4ZpkT",
        "colab_type": "text"
      },
      "source": [
        "#LibiumNet: Lip-Reading using RCNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkH0NM_xmmE7",
        "colab_type": "code",
        "outputId": "d50b97dd-2bf2-4011-c622-f0913aed1b3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "\n",
        "import os, glob\n",
        "import imageio\n",
        "import itertools\n",
        "import math\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "imageio.plugins.ffmpeg.download()\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Imageio: 'ffmpeg-linux64-v3.3.1' was not found on your computer; downloading it now.\n",
            "Try 1. Download from https://github.com/imageio/imageio-binaries/raw/master/ffmpeg/ffmpeg-linux64-v3.3.1 (43.8 MB)\n",
            "Downloading: 8192/45929032 bytes (0.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b3416064/45929032 bytes (7.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b7618560/45929032 bytes (16.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11804672/45929032 bytes (25.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15949824/45929032 bytes (34.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20004864/45929032 bytes (43.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b24158208/45929032 bytes (52.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28295168/45929032 bytes (61.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32522240/45929032 bytes (70.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b36683776/45929032 bytes (79.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b40370176/45929032 bytes (87.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b44605440/45929032 bytes (97.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45929032/45929032 bytes (100.0%)\n",
            "  Done\n",
            "File saved as /root/.imageio/ffmpeg/ffmpeg-linux64-v3.3.1.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K13X7Hdg3Ktp",
        "colab_type": "code",
        "outputId": "e49cada0-fe55-4727-f1f2-a68624798d08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "device = 'cpu'\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')\n",
        "    device = 'cuda'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA is available!  Training on GPU ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7_73zWRZHzw",
        "colab_type": "code",
        "outputId": "e441e4b2-bdfc-4b39-8131-052ee2693f50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "# mounting notebook to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-fZdjfygGXe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\"\"\"\n",
        "This model generates generator of the datasets for the Network. \n",
        "\n",
        "@authors : Mustapha Tidoo Yussif, Samuel Atule, Jean Sabastien Dovonon\n",
        "         and Nutifafa Amedior. \n",
        "\"\"\"\n",
        "IMAGE_HEIGHT = 256\n",
        "IMAGE_WIDTH = 256\n",
        "IMAGE_CHANNEL = 3\n",
        "NUM_FRAMES = 29\n",
        "NUM_CLASSES = 4\n",
        "        \n",
        "        \n",
        "class GenerateDataset(object):\n",
        "    \"\"\"Generates generator for the datasets\n",
        "    \n",
        "    This model generates a generator for the datasets. This done to efficiently \n",
        "    manage space.\n",
        "    \n",
        "    :param: file_path: path to files/videos.\n",
        "    :param directory: Path to the main directory.\n",
        "    \"\"\"\n",
        "    def __init__(self, file_path, directory, n_items):\n",
        "        self.n_items = n_items\n",
        "        self.directory = directory\n",
        "        self.file_path = file_path\n",
        "        self.num_samples = len(self.samples(self.get_video_files(self.file_path, self.directory)))\n",
        "        \n",
        "\n",
        "    def load_video(self, filename):\n",
        "        \"\"\"Loads the specified video using ffmpeg.\n",
        "\n",
        "        Returns:\n",
        "            List[FloatTensor]: the frames of the video as a list of 3D tensors\n",
        "                (channels, width, height)\"\"\"\n",
        "        \n",
        "        reader = imageio.get_reader(filename,  'ffmpeg')\n",
        "        \n",
        "        return np.array(list(reader), dtype=np.float32)\n",
        "    \n",
        "    def resize_frames(self, frames):\n",
        "        \"\"\"\n",
        "        Crops the frames of the videos around the mouth region.\n",
        "        This is the part that is most important part and relevant\n",
        "        to the model (where we can get the relevant features)\n",
        "\n",
        "        :param frames: The frames in the video. \n",
        "        :return: returns the croped frames.\n",
        "        \"\"\"\n",
        "        tf.image.resize_images(X, (IMAGE_SIZE, IMAGE_SIZE), \n",
        "                                    tf.image.ResizeMethod.NEAREST_NEIGHBOR) \n",
        "\n",
        "    def get_sample_size(self):\n",
        "      return self.num_samples\n",
        "    \n",
        "    \n",
        "    def create_df(self, file_path):\n",
        "        '''\n",
        "        creates pandas dataframe of labels and words directories\n",
        "        '''\n",
        "        \n",
        "        d = {}\n",
        "        y_labels = []\n",
        "        class_folders = []\n",
        "        for ind, clss in enumerate(os.listdir(file_path)):\n",
        "            y_labels.append(ind)\n",
        "            class_folders.append(clss)\n",
        "        \n",
        "        d['directory'] = class_folders\n",
        "        d['class'] = y_labels\n",
        "        return pd.DataFrame(d)\n",
        "\n",
        "\n",
        "    def get_video_files(self, file_path, directory=None):\n",
        "        '''\n",
        "        get video files from word class directories\n",
        "        '''\n",
        "        d = {}\n",
        "        f = []\n",
        "        \n",
        "        for root, dirs, files in os.walk(file_path):\n",
        "            if root.split('/')[-1] == directory:\n",
        "                for file in files:\n",
        "                    if file.endswith(\".mp4\"):\n",
        "                        target_file = file.split('_')[0]\n",
        "                        f.append(target_file)\n",
        "                        if target_file not in d:\n",
        "                            d[target_file] = []\n",
        "                        d[target_file].append(os.path.join(root, file))\n",
        "                    \n",
        "        return d\n",
        "        \n",
        "    def generator(self, batch = 1):\n",
        "        \"\"\"Interfaces the private generator method\n",
        "\n",
        "        :param num_items_per_class: The number of items in a categority. \n",
        "        :param batch: The batch size.\n",
        "        \"\"\"\n",
        "        data = self.create_df(self.file_path)\n",
        "        video_files = self.get_video_files(self.file_path, self.directory)\n",
        "        return self._generator(data, directory = self.directory, video_files = video_files, BATCH_SIZE = batch)\n",
        "\n",
        "    def samples(self, video_files):\n",
        "      train = []\n",
        "      for key, value in video_files.items():\n",
        "        ind = 0\n",
        "        for file in value:\n",
        "          train.append(file)\n",
        "          ind+=1\n",
        "          if ind == self.n_items:\n",
        "            break\n",
        "          \n",
        "      return train\n",
        "    \n",
        "    def _generator(self, data, directory=None, video_files=None, BATCH_SIZE = 64):\n",
        "        \n",
        "        '''\n",
        "        retrieves the training batch for each iteration\n",
        "        '''\n",
        "        \n",
        "        train = []\n",
        "        for key, value in video_files.items():\n",
        "            ind = 0\n",
        "            for file in value:\n",
        "                train.append(file)\n",
        "                ind+=1\n",
        "                if ind == self.n_items:\n",
        "                  break\n",
        "                \n",
        "                  \n",
        "                \n",
        "        while True:\n",
        "            # Randomize the indices to make an array\n",
        "            indices_arr = np.random.permutation(len(train))\n",
        "            \n",
        "            for batch in range(0, len(indices_arr), BATCH_SIZE):\n",
        "                # slice out the current batch according to batch-size\n",
        "                current_batch = indices_arr[batch:(batch + BATCH_SIZE)]\n",
        "\n",
        "                # initializing the arrays, x_train and y_train\n",
        "                x_train = np.empty([0, NUM_FRAMES, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL], dtype=np.float32)\n",
        "            \n",
        "                y_train = np.empty([0], dtype=np.int32)\n",
        "\n",
        "                for i in current_batch:\n",
        "                    # get an image and its corresponding color for an traffic light\n",
        "                    video_frames = self.load_video(train[i])\n",
        "                    \n",
        "                    \n",
        "                    #preprocess frames from videos\n",
        "#                     video_frames = tf.image.resize_nearest_neighbor(video_frames,(IMAGE_HEIGHT, IMAGE_WIDTH), )\n",
        "                    #video_frames = tf.image.rgb_to_grayscale(video_frames)\n",
        "#                     video_frames = tf.reshape(video_frames, (NUM_FRAMES, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL))\n",
        "\n",
        "                    # Appending them to existing batch\n",
        "                    x_train = np.append(x_train, [video_frames/255], axis=0)\n",
        "                    y_train = np.append(y_train, [ data.loc[ data['directory'] == train[i].split('/')[-1].split('_')[-2] ].values[0][1] ])\n",
        "                    #print(data.loc[ data['directory'] == train[i].split('/')[-1].split('_')[-2] ].values[0][1])\n",
        "                    \n",
        "                \n",
        "                y_train = to_categorical(y_train, num_classes=NUM_CLASSES)\n",
        "\n",
        "                yield torch.autograd.Variable(torch.from_numpy(x_train)), torch.autograd.Variable(torch.from_numpy(y_train))\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46N4_yDpA1Mc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = GenerateDataset('/gdrive/My Drive/LibiumNet/lipread_mp4/', 'train', 2)\n",
        "datasets = train_loader.generator()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rCl5NsX8egf",
        "colab_type": "code",
        "outputId": "0e7e8056-2396-4fc9-e7cf-a36ac1b54359",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "next(datasets)[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 1., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYYcQtph-YJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class C3D(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(C3D, self).__init__()\n",
        "        self.group1 = nn.Sequential(\n",
        "            nn.Conv3d(3, 64, kernel_size=5, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)))\n",
        "        \n",
        "        self.group2 = nn.Sequential(\n",
        "            nn.Conv3d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)))\n",
        "        \n",
        "        \n",
        "\n",
        "        self._features = nn.Sequential(\n",
        "            self.group1,\n",
        "            self.group2\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view((1, 3, 256, 256, 29))\n",
        "        return self._features(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eyHlWVBAkpI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, nChannels, growthRate):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        interChannels = 4*growthRate\n",
        "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
        "        self.conv1 = nn.Conv2d(nChannels, interChannels, kernel_size=1,\n",
        "                               bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(interChannels)\n",
        "        self.conv2 = nn.Conv2d(interChannels, growthRate, kernel_size=3,\n",
        "                               padding=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(F.relu(self.bn1(x)))\n",
        "        out = self.conv2(F.relu(self.bn2(out)))\n",
        "        out = torch.cat((x, out), 1)\n",
        "        return out\n",
        "\n",
        "class SingleLayer(nn.Module):\n",
        "    def __init__(self, nChannels, growthRate):\n",
        "        super(SingleLayer, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
        "        self.conv1 = nn.Conv2d(nChannels, growthRate, kernel_size=3,\n",
        "                               padding=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(F.relu(self.bn1(x)))\n",
        "        out = torch.cat((x, out), 1)\n",
        "        return out\n",
        "\n",
        "class Transition(nn.Module):\n",
        "    def __init__(self, nChannels, nOutChannels):\n",
        "        super(Transition, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
        "        self.conv1 = nn.Conv2d(nChannels, nOutChannels, kernel_size=1,\n",
        "                               bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(F.relu(self.bn1(x)))\n",
        "        out = F.avg_pool2d(out, 2)\n",
        "        return out\n",
        "\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "    def __init__(self, growthRate=40, depth=10, reduction=1, bottleneck=True):\n",
        "        super(DenseNet, self).__init__()\n",
        "\n",
        "        nDenseBlocks = (depth-4) // 3\n",
        "        if bottleneck:\n",
        "            nDenseBlocks //= 2\n",
        "\n",
        "        nChannels = 2*growthRate\n",
        "        self.conv1 = nn.Conv2d(128, nChannels, kernel_size=3, padding=1,\n",
        "                               bias=False)\n",
        "        self.dense1 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
        "        nChannels += nDenseBlocks*growthRate\n",
        "        nOutChannels = int(math.floor(nChannels*reduction))\n",
        "        self.trans1 = Transition(nChannels, nOutChannels)\n",
        "\n",
        "        nChannels = nOutChannels\n",
        "        self.dense2 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
        "        nChannels += nDenseBlocks*growthRate\n",
        "        nOutChannels = int(math.floor(nChannels*reduction))\n",
        "        self.trans2 = Transition(nChannels, nOutChannels)\n",
        "\n",
        "        nChannels = nOutChannels\n",
        "        self.dense3 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
        "        nChannels += nDenseBlocks*growthRate\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_dense(self, nChannels, growthRate, nDenseBlocks, bottleneck):\n",
        "        layers = []\n",
        "        for i in range(int(nDenseBlocks)):\n",
        "            if bottleneck:\n",
        "                layers.append(Bottleneck(nChannels, growthRate))\n",
        "            else:\n",
        "                layers.append(SingleLayer(nChannels, growthRate))\n",
        "            nChannels += growthRate\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.trans1(self.dense1(out))\n",
        "        out = self.trans2(self.dense2(out))\n",
        "        out = self.dense3(out)\n",
        "        out = F.avg_pool2d(F.relu(self.bn1(out)), kernel_size=3)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i2xKaZYqalc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class block(nn.Module):\n",
        "    def __init__(self,ni):\n",
        "        super(block, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(ni, ni, 1)\n",
        "        self.conv2 = nn.Conv1d(ni, ni, 3, 1, 1)\n",
        "\n",
        "    def forward(self,x):\n",
        "        residual = x\n",
        "        out = F.relu(self.conv1(x))\n",
        "        out = F.relu(self.conv2(out))\n",
        "        \n",
        "        \n",
        "        \n",
        "        out += residual\n",
        "        \n",
        "        \n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AjfM1MoDUTy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SEQ(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1):\n",
        "        super(SEQ, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size1\n",
        "\n",
        "        self.c1 = block(input_size)\n",
        "        self.p1 = nn.AvgPool2d(2)\n",
        "        self.c2 = block(hidden_size1)\n",
        "        self.p2 = nn.AvgPool1d(2)\n",
        "        self.c3 = block(hidden_size1)\n",
        "        self.p3 = nn.AvgPool1d(2)\n",
        "        self.c4 = block(hidden_size1)\n",
        "        self.p4 = nn.AvgPool1d(2)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "      \n",
        "        #print(inputs.shape)\n",
        "\n",
        "        # Run through Conv1d and Pool1d layers\n",
        "        c = self.c1(inputs)\n",
        "        p = self.p1(c)\n",
        "        c = self.c2(p)\n",
        "        p = self.p2(c)\n",
        "        c = self.c3(p)\n",
        "        p = self.p3(c)\n",
        "        c = self.c4(p)\n",
        "        p = self.p4(c)\n",
        "        \n",
        "        \n",
        "        \n",
        "        out = F.relu(p)\n",
        "        out = out.view(out.size(0), out.size(1))\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaiNeeZ84OYF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def flatten(t):\n",
        "    t = t.reshape(1, -1)\n",
        "    t = t.squeeze()\n",
        "    return t\n",
        "  \n",
        "class TimeDistributed(nn.Module):\n",
        "    def __init__(self, module, batch_first=True):\n",
        "        super(TimeDistributed, self).__init__()\n",
        "        self.module = module\n",
        "        self.batch_first = batch_first\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        tList = [flatten(self.module(m)) for m in torch.unbind(x, dim=4) ]\n",
        "        y = torch.stack(tList, dim=0)\n",
        "        # We have to reshape Y\n",
        "        if self.batch_first:\n",
        "            y = y.contiguous().view(x.size(0), y.size(-1), -1)  # (samples, timesteps, output_size)\n",
        "        else:\n",
        "            y = y.view(-1, x.size(1), y.size(-1))  # (timesteps, samples, output_size)\n",
        "        return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czRPcoYVxSLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_classes = 4\n",
        "\n",
        "model = nn.Sequential(OrderedDict([\n",
        "    ('frontend', C3D()),\n",
        "    ('features', TimeDistributed(DenseNet())),\n",
        "    ('backend', SEQ(input_size=5000, hidden_size1=2500)),\n",
        "    ('fc', nn.Sequential( nn.Dropout(p=0.5), nn.Linear(2500, n_classes) ))\n",
        "]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qwCj8Li71iY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## function to train the network\n",
        "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    train_steps = train_loader.get_sample_size()\n",
        "    \n",
        "    gen = train_loader.generator()\n",
        "    \n",
        "    for batch_idx in tqdm(range(train_steps)): \n",
        "        data, target = next(gen)\n",
        "        data, target = data.to(device), torch.max(target.long().to(device), 1)[1]\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        \n",
        "    train_loss /= train_loader.get_sample_size()\n",
        "    print('Epoch: {} , Training Accuracy: {}/{} ({:.0f}%) Training Loss: {:.6f}'.format(\n",
        "                epoch, correct, train_loader.get_sample_size(),\n",
        "                100. * correct / train_loader.get_sample_size(), train_loss))\n",
        "\n",
        "## function to train the network\n",
        "def test(model, device, criterion, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    steps = test_loader.get_sample_size()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(test_loader.generator()):\n",
        "            data, target = data.to(device), torch.max(target.long().to(device), 1)[1]\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()\n",
        "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            if batch_idx >= steps-1:\n",
        "              break\n",
        "\n",
        "    test_loss /= test_loader.get_sample_size()\n",
        "\n",
        "    print('Test loss: {:.4f}, Test Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, test_loader.get_sample_size(),\n",
        "        100. * correct / test_loader.get_sample_size()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXBeZcXl-DgO",
        "colab_type": "code",
        "outputId": "65262792-b672-4326-a6d2-46a9a5dad4b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "import torch.optim as optim\n",
        "# specify loss function (categorical cross-entropy)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# specify optimizer and learning rate\n",
        "optimizer = optim.SGD(\n",
        "  [\n",
        "        {\"params\": model.fc.parameters(), \"lr\": 1e-3},\n",
        "        {\"params\": model.backend.parameters(), \"lr\": 1e-5},\n",
        "        {\"params\": model.features.parameters(), \"lr\": 1e-4},\n",
        "        {\"params\": model.frontend.parameters(), \"lr\": 1e-4},\n",
        "  ],\n",
        "  momentum = 0.9\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-36d14cd721c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# specify loss function (categorical cross-entropy)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# specify optimizer and learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m optimizer = optim.SGD(\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y58_FQXpt23N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = GenerateDataset('/gdrive/My Drive/LibiumNet/lipread_mp4/', 'train',200)\n",
        "test_loader = GenerateDataset('/gdrive/My Drive/LibiumNet/lipread_mp4/', 'val',20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSG5B1fguRha",
        "colab_type": "code",
        "outputId": "dd9df604-ecf0-4b61-9ee6-61f0c1fa3f62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1411
        }
      },
      "source": [
        "# number of epochs to train the model\n",
        "n_epochs = 30\n",
        "\n",
        "if train_on_gpu:\n",
        "    model.cuda()\n",
        "\n",
        "print(\"Start training \\n\\n\")\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "  \n",
        "    \n",
        "    train (model, device, train_loader, optimizer, criterion, epoch)\n",
        "    test (model, device, criterion, test_loader)\n",
        "    \n",
        "    print ('\\n')\n",
        "            \n",
        "print (\"Done training\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/800 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Start training \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 800/800 [17:53<00:00,  1.44s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 , Training Accuracy: 189/800 (24%) Training Loss: 3.064293\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/800 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test loss: 15.5779, Test Accuracy: 20/80 (25%)\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 800/800 [13:48<00:00,  1.05s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 2 , Training Accuracy: 210/800 (26%) Training Loss: 2.731881\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/800 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test loss: 7.1115, Test Accuracy: 18/80 (22%)\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 800/800 [13:47<00:00,  1.03s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 3 , Training Accuracy: 190/800 (24%) Training Loss: 2.410185\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/800 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test loss: 7.4882, Test Accuracy: 21/80 (26%)\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 800/800 [13:47<00:00,  1.03s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 4 , Training Accuracy: 188/800 (24%) Training Loss: 2.212992\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/800 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test loss: 8.4877, Test Accuracy: 20/80 (25%)\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 800/800 [13:49<00:00,  1.04s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 5 , Training Accuracy: 200/800 (25%) Training Loss: 2.078209\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/800 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test loss: 8.1017, Test Accuracy: 20/80 (25%)\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 800/800 [13:47<00:00,  1.04s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 6 , Training Accuracy: 220/800 (28%) Training Loss: 1.848193\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/800 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test loss: 3.6153, Test Accuracy: 21/80 (26%)\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 800/800 [13:48<00:00,  1.03s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 7 , Training Accuracy: 219/800 (27%) Training Loss: 1.833462\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/800 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test loss: 3.4465, Test Accuracy: 18/80 (22%)\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 800/800 [13:47<00:00,  1.03s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 8 , Training Accuracy: 238/800 (30%) Training Loss: 1.777003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/800 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test loss: 2.0547, Test Accuracy: 21/80 (26%)\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 800/800 [13:46<00:00,  1.03s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 9 , Training Accuracy: 282/800 (35%) Training Loss: 1.649221\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/800 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test loss: 3.2229, Test Accuracy: 19/80 (24%)\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 800/800 [13:45<00:00,  1.03s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 10 , Training Accuracy: 284/800 (36%) Training Loss: 1.546823\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/800 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test loss: 3.8496, Test Accuracy: 24/80 (30%)\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 800/800 [13:46<00:00,  1.04s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 11 , Training Accuracy: 305/800 (38%) Training Loss: 1.531270\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/800 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test loss: 1.8881, Test Accuracy: 19/80 (24%)\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 800/800 [13:46<00:00,  1.02s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 12 , Training Accuracy: 337/800 (42%) Training Loss: 1.388208\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/800 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test loss: 2.7919, Test Accuracy: 22/80 (28%)\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 800/800 [13:48<00:00,  1.05s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 13 , Training Accuracy: 367/800 (46%) Training Loss: 1.340048\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/800 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test loss: 8.6739, Test Accuracy: 18/80 (22%)\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 56%|█████▌    | 448/800 [07:43<06:07,  1.04s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NUyGGyPuZVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), 'densefullmodel2.pwf')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5J9_H9qmin5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}